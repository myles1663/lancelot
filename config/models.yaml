# ===========================================================================
# Lancelot v4 â€” Model Configuration
# ===========================================================================
# Runtime-loaded provider profiles for the lane-based routing system.
# Each flagship provider maps to fast, deep, and optional cache lanes.
# The local utility model is managed separately by the lockfile.
#
# Version: 1.0

version: "1.0"

# ---------------------------------------------------------------------------
# Local utility model (managed by models.lock.yaml)
# ---------------------------------------------------------------------------
local:
  enabled: true
  url: "http://local-llm:8080"

# ---------------------------------------------------------------------------
# Flagship provider profiles
# ---------------------------------------------------------------------------
# The active provider is selected during onboarding (FLAGSHIP_SELECTION).
# Each provider defines lanes used by the ModelRouter.

providers:
  gemini:
    display_name: "Google Gemini"
    mode: "sdk"
    fast:
      model: "gemini-3-flash-preview"
      max_tokens: 8192
      temperature: 0.3
    deep:
      model: "gemini-3-pro-preview"
      max_tokens: 16384
      temperature: 1.0
    cache:
      model: "gemini-2.5-flash"
      max_tokens: 2048
      temperature: 0.1

  openai:
    display_name: "OpenAI"
    mode: "sdk"
    fast:
      model: "gpt-4o-mini"
      max_tokens: 4096
      temperature: 0.3
    deep:
      model: "gpt-4o"
      max_tokens: 8192
      temperature: 0.7
    cache:
      model: "gpt-4o-mini"
      max_tokens: 2048
      temperature: 0.1

  anthropic:
    display_name: "Anthropic"
    mode: "sdk"
    fast:
      model: "claude-sonnet-4-5-20250929"
      max_tokens: 8192
      temperature: 0.3
    deep:
      model: "claude-opus-4-6"
      max_tokens: 16384
      temperature: 1.0
      thinking:
        enabled: true
        budget_tokens: 10000
    cache:
      model: "claude-haiku-4-5-20251001"
      max_tokens: 2048
      temperature: 0.1

  xai:
    display_name: "xAI (Grok)"
    mode: "sdk"
    fast:
      model: "grok-3-mini"
      max_tokens: 4096
      temperature: 0.3
    deep:
      model: "grok-3"
      max_tokens: 16384
      temperature: 0.7
    cache:
      model: "grok-3-mini"
      max_tokens: 2048
      temperature: 0.1
