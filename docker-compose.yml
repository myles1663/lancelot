services:
  lancelot-core:
    build: .
    container_name: lancelot_core
    volumes:
      - ./lancelot_data:/home/lancelot/data
      # Map the current directory to /home/lancelot/app for development
      - .:/home/lancelot/app
      # Docker socket for sandbox provider (sibling containers)
      - /var/run/docker.sock:/var/run/docker.sock
      # Shared workspace folder — user and Lancelot can both read/write
      - "C:\\Users\\SSAdministrator\\Desktop\\Lancelot Workspace:/home/lancelot/workspace"
      # Mount Google OAuth credentials (ADC) from host (uncomment if using Google OAuth)
      # - ${APPDATA}/gcloud/application_default_credentials.json:/home/lancelot/.config/gcloud/application_default_credentials.json:ro
    group_add:
      - "0"  # root group to access Docker socket (owned root:root on Docker Desktop)
    extra_hosts:
      - "host.docker.internal:host-gateway"
    env_file:
      - .env
    environment:
      - PYTHONUNBUFFERED=1
      # - GOOGLE_APPLICATION_CREDENTIALS=/home/lancelot/.config/gcloud/application_default_credentials.json
      - LOCAL_LLM_URL=http://local-llm:8080
      - LANCELOT_WORKSPACE=/home/lancelot/workspace
      - PYTHONPATH=/home/lancelot/app/src/core:/home/lancelot/app/src/ui:/home/lancelot/app/src/agents:/home/lancelot/app/src/memory:/home/lancelot/app/src/shared:/home/lancelot/app/src/integrations:/home/lancelot/app/src:/home/lancelot/app
      # Host Agent bridge — set HOST_AGENT_TOKEN in .env to match host_agent
      - HOST_AGENT_URL=http://host.docker.internal:9111
      - HOST_AGENT_TOKEN=${HOST_AGENT_TOKEN:-lancelot-host-agent}
    depends_on:
      local-llm:
        condition: service_healthy
    ports:
      - "8000:8000"
    command: uvicorn gateway:app --host 0.0.0.0 --port 8000
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      start_period: 60s
      retries: 3
    restart: unless-stopped
    networks:
      - lancelot_net

  local-llm:
    build:
      context: ./local_models
      dockerfile: Dockerfile
    container_name: lancelot_local_llm
    volumes:
      # Model weights — downloaded during onboarding, never baked into image
      - ./local_models/weights:/home/llm/models:ro
    environment:
      - LOCAL_MODELS_DIR=/home/llm/models
      - LOCAL_MODEL_CTX=4096
      - LOCAL_MODEL_THREADS=4
      - LOCAL_MODEL_GPU_LAYERS=15
      - LOCAL_LLM_PORT=8080
    # Fix Pack V8: GPU access for CUDA offload
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      start_period: 120s
      retries: 3
    restart: unless-stopped
    networks:
      - lancelot_net

networks:
  lancelot_net:
    driver: bridge
